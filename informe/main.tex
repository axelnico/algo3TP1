\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc} % para poder usar tildes en archivos UTF-8
\usepackage[spanish]{babel} % para que comandos como \today den el resultado en castellano
\usepackage{a4wide} % márgenes un poco más anchos que lo usual
\usepackage{caratula}
% \usepackage[left=3cm,right=3cm,bottom=3cm,top=3cm]{geometry}

% Comandos para simbolos matematicos.
\usepackage{amsmath, amssymb, tabularx}

% Comandos para referencias
\usepackage{natbib}

% Comandos para Figuras, Graficos, Tikz etc.
\usepackage{tikz}
\usepackage{epsfig}
\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{svg}
\setsvg{inkscape=inkscape -z -D}

% Comandos para teoremas etc.
\usepackage{amsthm}
\newtheorem{theorem}{Teorema}
\newtheorem{lemma}[theorem]{Lema}
\newtheorem{proposition}[theorem]{Proposición}
\newtheorem{remark}{Observación}
\newtheorem{corollary}{Corolario}
% \newproof{proof}{Demostración}

% Comandos para algoritmos.
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}
\algnewcommand{\IfThenElse}[3]{% \IfThenElse{<if>}{<then>}{<else>}
\State \algorithmicif\ #1\ \algorithmicthen\ #2\ \algorithmicelse\ #3}
\algnewcommand{\IfThen}[2]{% \IfThenElse{<if>}{<then>}
  \State \algorithmicif\ #1\ \algorithmicthen\ #2}
\pgfplotsset{compat=1.16}

\begin{document}

\titulo{TP1 - Optimizando Jambo-tubos}

\fecha{}

\materia{Algoritmos y estructuras de datos III}

\integrante{Bukovits, Nicolás Axel}{546/14}{nicko\_buk@hotmail.com}
\integrante{Chen, Alejandro Antonio}{507/17}{chenalejandro@outlook.com}
\maketitle
\thispagestyle{empty}

\newpage

\setcounter{page}{1}

\section{Introducción} \label{sec:introduccion}

La cadena de supermercados Jambo quiere construir un robot con el fin de optimizar las operaciones en sus locales. Uno de ellos es el servicio de empacado de productos en los denominados jambos-tubos, donde en cada tubo se apilan los distintos productos, uno arriba del otro, es decir, un producto no puede estar ubicado al lado de otro, solamente arriba o abajo.\\

Como es de conocimiento general, hay ciertos productos que si tienen cierto peso por encima se aplastan, lo cual es una situaci\'on indeseable. Por lo cual, vamos a tener que resolver este problema decidiendo qu\'e productos apilar para no aplastar los otros que estan abajo. Entonces, sabiendo el orden de la llegada de los productos y sus respectivos pesos y resistencias, es decir, cu\'anto peso pueden tener arriba sin ser aplastados, decidir cu\'ales de los productos poner en el tubo. Observar que el tubo tiene tambi\'en su resistencia propia, o sea que tiene un l\'imite de peso total que se puede cargar.\\

Vamos a llamar $R$ a la resistencia del tubo, $n$ a la cantidad de productos, $S$ a la secuencia ordenada de $n$ productos, $w_i$ al peso de cada producto y $r_i$ a sus respectivos resistencias asociados. El problema consiste en determinar la máxima cantidad de productos que pueden apilarse en el tubo, sin que ninguno esté aplastado. Notar que se debe respetar el orden dado y que todos los valores mencionados son enteros no negativos. Por ejemplo, dado un tubo con $R = 50$, $n = 5$, $w = [10, 20, 30, 10, 15]$ y $r = [45, 8, 15, 2, 30]$, la soluci\'on \'optima es 3, y consiste en tomar los elementos 1, 3 y 4. Observar que tomando los elementos 1, 3 y 5 no es una soluci\'on factible ya que la suma de sus pesos es $55 > R$. Otro ejemplo, en donde no hay una solución, puede ser con  $R = 50$, $n = 3$, $w = [55, 65, 60]$ y $r = [20, 10, 5]$. En este caso no se puede agregar ningún producto, porque se aplastan y porque todos pesan más que la resistencia del tubo, por lo que la respuesta al problema es $0$.     \\

La finalidad de este trabajo es resolver el problema presentado de los jambo-tubos, utilizando tres t\'ecnicas de programaci\'on distintas. La primera consiste en un algoritmo de fuerza bruta recursivo, que enumera todas las posibles soluciones qued\'andose con la mejor posible de las factibles. La segunda t\'ecnica consiste en un algoritmo de backtracking implementado con podas para reducir la cantidad de nodos del \'arbol con el objetivo de que sea m\'as efeciente. Y finalmente se presentar\'a un algoritmo recursivo de programaci\'on din\'amica, utilizando memoizaci\'on para evitar repetir los c\'omputos de los subproblemas. Posteriormente se realizará una experimentación comparando estos tres algoritmos con distintos tipos de instancias para ver como se comportan en distintas situaciones. Se cuenta para todos los algoritmos con un vector de longitud $n$, denominado $productos$ que tiene en cada posicion una estructura que representa a un producto con su peso y resistencia. Asi, por ejemplo el peso y la resistencia del producto $i$ se accede con $productos[i].peso$ y $productos[i].resistencia$ respectivamente.

\section{Fuerza Bruta} \label{sec:fuerza_bruta}
La t\'ecnica algorítmica de  Fuerza Bruta consiste en enumerar todo el conjunto de soluciones en búsqueda de aquellas factibles u óptimas según si el problema es de decisión u optimización. En este caso, se trata de un problema donde se busca el número máximo de productos que se pueden apilar, dadas unas restricciones. El conjunto de soluciones está compuesto entonces por todos los subconjuntos de $S$, es decir, es el \emph{conjunto de partes de $S$} que se escribe $\mathcal{P}(S)$.

La idea del Algoritmo~\ref{alg:fuerza_bruta} para resolver el problema del jambo-tubo es ir generando las soluciones de manera recursiva decidiendo en cada paso si un producto de $S$ es agregado o no y quedándose con la mejor solución (el máximo) de alguna de las dos ramas. Cuando se llega a una solución (que es cuando se analizan todos los productos, hayan o no hayan sido agregados), se verifica que la solución es válida, es decir, cumple con los requisitos del problema, en este caso que ningún producto se aplaste y que la suma de los pesos $w_i$ no supere la resistencia del tubo $R$. En caso de sér válida se devuelve la cantidad de productos apilados. En caso de no ser válida se devuelve 0. Para este algoritmo se usa una variable global denominada $solucionParcial$, que es un vector de valores booleanos de longitud $n$ inicializado completamente en $false$, que se utiliza para saber si un producto fue o no agregado. Cuando es agregado el producto $i$ se almacena en la posición $solucionParcial[i]$ el valor $true$, caso contrario el valor $false$.

En la Figura~\ref{fig:ejemplo_fuerza_bruta} se ve un ejemplo del árbol de recursión para la instancia $S =\{1, 2, 3\}$ y $W=3$. Cada nodo intermedio del árbol representa una \emph{solución parcial}, es decir, cuando aún no se tomaron todas las decisiones de qué elementos incluir, mientras que las hojas representan a todas las soluciones (8 en este caso). La solución óptima $\{3\}$ está marcada en rojo y la otra solución factible $\{1, 2\}$ en gris. Notar que la solución al problema original es exactamente $FB(S, W, 0, 0, 0)$.

\begin{algorithm}
\begin{algorithmic}[1]
\Function{$FB$}{$i$, $k$, $p$}
    \If{$i = n$}
        \IfThenElse{$esSolucionValida(p)$}{\textbf{return} $k$}{\textbf{return} $0$}
    \EndIf
    \State {$solucionParcial[i] \gets false$}
    \State {$sinAgregar \gets FB(i+1, k, p)$}
    \State {$solucionParcial[i] \gets true$}
    \State {$agregado \gets FB(i+1, k + 1, p + productos[i].peso)$}
    \State \textbf{return} $\max \{ sinAgregar,agregado \}$.
\EndFunction
\end{algorithmic}
\caption{Algoritmo de Fuerza Bruta para Jambo-tubos.}
\label{alg:fuerza_bruta}
\end{algorithm}

La correctitud del algoritmo se debe a que se generan todas las posibles soluciones, dado que para cada elemento de $S$ se crean dos ramas una agregandólo al producto y la otra no agregándolo. Cuando no se agrega al producto simplemente se pasa al siguiente. Cuando es agregado el producto, se incrementa en uno el valor de $k$ que representa la cantidad de productos agregados, y se agrega también el peso del producto agregado a $p$, que contiene el peso acumulado de los productos agregados. Al haber generado todas las posibles soluciones, debe encontrarse la óptima (de existir), caso contrario el valor del óptimo será $0$, ya que no es posible apilar ningun próducto. La óptima va a ser encontrada mediante la llamada a la funcion $max$ que calcula el valor máximo entre las dos ramas consideradas. El resultado al problema se obtiene llamando a la función $FB$($0$, $0$, $0$).

La complejidad del Algoritmo~\ref{alg:fuerza_bruta} para el peor caso es $O(n*2^n)$. Esto se debe a que el árbol de recursión es un árbol binario completo de $n+1$ niveles (contando la raíz), dado que cada nodo se ramifica en dos hijos y en cada paso el parámetro $i$ es incrementado en 1 hasta llegar a $n$. La solución de cada llamado recursivo toma tiempo constante dado que se realizan solo sumas, asignaciones a vectores y comparaciones, excepto el caso cuando $i = n$. En este caso, que correspondería a las hojas del árbol, cuando se terminaron de analizar todos los productos, se llama a una función cuyo costo es de $O(n)$. El pseudocódigo de la función es el siguiente:

\begin{algorithm}
\begin{algorithmic}[1]
\Function{$esSolucionValida$}{$p$}
    \State \textbf{return} todosLosProductosResistenPeso($n$, $p$) \textbf{and} $p$ $<=$ $R$.
\EndFunction

\Function{$todosLosProductosResistenPeso$}{$i$, $p$}
    \State $j$ = $0$
    \State $pesoEncima$ = $p$
    \While{$j$ $<$ $i$ \textbf{and} $resistePeso$($j$, $pesoEncima$)} \Comment{corta si producto $j$ se aplasta}
        \If{$solucionParcial(j)$}
            \State $pesoEncima \gets pesoEncima - productos[j].peso$
        \EndIf
        \State $j++$
    \EndWhile
    \State \textbf{return} $j$ $=$ $i$
\EndFunction

\Function{$resistePeso$}{$i$, $p$}
    \If{$solucionParcial(i)$}
            \State \textbf{return} $p$ $-$ $productos[i].peso$ $<=$ $productos[i].resistencia$
        \EndIf 
    \State \textbf{return} $true$
\EndFunction

\end{algorithmic}
\caption{funciones para verificar si una solución encontrada es válida}
\end{algorithm}

De la función expuesta arriba se puede deducir que la complejidad de la función que determina si los productos resisten el peso encima ($todosLosProductosResistenPeso$) es $O(n)$ en el peor caso, ya que se realizan todas operaciones en tiempo constante, y sólo hay un ciclo que en el peor caso itera $n$ veces (cuando es llamada con el parámetro $n$). Notar que el ciclo corta ni bien encuentra un producto que no resiste el peso encima de el. Para saber cuál es el peso encima de cada producto, al peso total recibido se le va restando el peso de cada producto y verificando que el peso restante no sea mayor a la resistencia del producto para el producto $i$. 

Con lo expuesto anteriormente, también se puede pensar que si bien la complejidad del algoritmo de fuerza bruta es $O(n*2^n)$, el algoritmo va a presentar una mejor performance en instancias en donde pocos productos puedan ser apilados (con el mejor caso siendo cuando ningún producto se puede apilar) y se va a comportar de una peor manera en tiempos de ejecución en instancias en donde varios productos se pueden apilar (con el peor caso siendo que todos los productos se puedan apilar). La causa de esta hipótesis es que el ciclo que verifica que los productos se pueden o no apilar, corta ni bien un producto no se puede apilar, es decir, no itera siempre hasta el final, por lo que si ningún producto se puede apilar, no itera ni una sola vez. Caso contrario, si todos se pueden apilar iterará hasta $n$. Obviamente para las soluciones candidatas en donde no se agregan productos, con el caso extremo de la solucion en donde no se considera agregar ninguno, la iteracion se realizará hasta $n$ ya que la función de $resistePeso$ devuelve verdadero si el producto no fue agregado. Esta hipótesis será puesta a prueba en el apartado de experimentación de este trabajo.

\begin{figure}
    \centering
    \begin{tikzpicture}[scale=0.4]
    \centering
        % Arcos
        \draw[color=black, -] (11, 6) -- node[above] {$\cup \emptyset$} (5, 4);
        \draw[color=black, -] (11, 6) -- node[above] {$\cup 1$} (17, 4);
        
        \draw[color=black, -] (5, 4) -- node[above left] {$\cup \emptyset$} (2, 2);
        \draw[color=black, -] (5, 4) -- node[above right] {$\cup 2$} (8, 2);
        \draw[color=black, -] (17, 4) -- node[above left] {$\cup \emptyset$} (14, 2);
        \draw[color=black, -] (17, 4) -- node[above right] {$\cup 2$} (20, 2);
        
        \draw[color=black, -] (2, 2) -- node[left] {$\cup \emptyset$} (0, 0);
        \draw[color=black, -] (2, 2) -- node[right] {$\cup 3$} (4, 0);
        \draw[color=black, -] (8, 2) -- node[left] {$\cup \emptyset$} (6, 0);
        \draw[color=black, -] (8, 2) -- node[right] {$\cup 3$} (10, 0);
        \draw[color=black, -] (14, 2) -- node[left] {$\cup \emptyset$} (12, 0);
        \draw[color=black, -] (14, 2) -- node[right] {$\cup 3$} (16, 0);
        \draw[color=black, -] (20, 2) -- node[left] {$\cup \emptyset$} (18, 0);
        \draw[color=black, -] (20, 2) -- node[right] {$\cup 3$} (22, 0);
        
        % Vertices
        \path (0,0) node[circle,draw,fill=white](A){};
        \path (4,0) node[circle,draw,fill=red](B){};
        \path (6,0) node[circle,draw,fill=white](C){};
        \path (10,0) node[circle,draw,fill=white](D){};
        \path (12,0) node[circle,draw,fill=white](E){};
        \path (16,0) node[circle,draw,fill=white](F){};
        \path (18,0) node[circle,draw,fill=gray](G){};
        \path (22,0) node[circle,draw,fill=white](H){};
        
        \path (2,2) node[circle,draw,fill=white](I){};
        \path (8,2) node[circle,draw,fill=white](J){};
        \path (14,2) node[circle,draw,fill=white](K){};
        \path (20,2) node[circle,draw,fill=white](L){};
        
        \path (5,4) node[circle,draw,fill=white](M){};
        \path (17,4) node[circle,draw,fill=white](N){};
        
        \path (11,6) node[circle,draw,fill=white](O){};
    
    \end{tikzpicture}
    \caption{Ejemplo de ejecución del Algoritmo~\ref{alg:fuerza_bruta} para $S = \{1, 2, 3\}$ y $W=3$.\\En rojo la solución óptima $\{3\}$ y en gris la otra solución factible.}
    \label{fig:ejemplo_fuerza_bruta}
\end{figure}

\section{Backtracking} \label{sec:backtracking}
La técnica de Backtracking consiste en ir generando solucion parciales, que se van extendiendo hasta llegar a una solución final, para enumerar todas las soluciones posibles. Es una idea similar a Fuerza bruta. De hecho el algoritmo presentando de Fuerza bruta sigue este idea, ya que Fuerza bruta se puede implementar con backtracking. La diferencia con Fuerza bruta radica en las denominadas \emph{podas} que son reglas que permiten evitar explorar partes del árbol que genera backtracking, en las que se \emph{sabe} que no va a existir ninguna solución de interés. Dicho de otra manera se \emph{poda} el árbol, por lo que se tienen que analizar menos nodos.  Generalmente estas podas dependen de cada problema en particular, pero las más comunes suelen dividirse en dos categorías: \emph{factibilidad} y \emph{optimalidad}. Para el caso del agoritmo propuesto en esta sección al problema de los jambo-tubos se utilizaron dos podas de factibilidad y una de optimalidad. 

\paragraph{Podas por factibilidad}
La primera poda por factibilidad es la siguiente. Sea $p$ el peso acumulado de los productos agregados en un nodo intermedio $n_0$. Si $p$ ya es mayor a $R$ entonces, no va a ver forma de extender la solución parcial a una solución válida debido a que todos los $w_i$ son positivos, por lo que el peso que se agregaría extendiendo una solución sería siempre mayor a 0 (o 0 en caso de que no se agregue ningun producto más). Por lo tanto podemos retornar 0 ya que la solución ya no es válida ni puede serlo más adelante. Esta poda está expresada en la línea \ref{linea:fact} del Algoritmo~\ref{alg:backtracking}.
La segunda poda por factibilidad es: Sea $p$ el peso acumulado de los productos agregados en un nodo intermedio $n_0$. Si existe un $producto_i$ que fue agregado y que no resiste el peso por encima de el (siendo el peso por encima de el, el peso total de los productos agregados hasta el nodo $n_0$ menos el peso del mismo producto $i$ y de todos los productos que están por debajo de $i$), entonces esta solución parcial tampoco puede ser extendida a una solución factible, ya que por más que no se agregue ningún producto más, va a seguir sin resistir el peso, y si se agrega algún producto más como todos los pesos son positivos, va a tener más peso por encima, por lo que tampoco va a resistir. Se puede entonces retornar y devolver 0. Esta poda está expresada en la línea \ref{linea:fact} del Algoritmo~\ref{alg:backtracking}.

\paragraph{Poda por optimalidad}
Para el caso de esta poda, se utiliza otra variable auxiliar global denominada $maxValue$. En esta variable se irá almacenando la mejor solución encontrada hasta el momento por el algoritmo de backtracking, es decir la máxima cantidad de productos que se pueden apilar. Entonces para los nodos intermedios $n_0$, que representa a una solución parcial, se realiza la siguiente comprobación. En la variable $k$ se tiene la cantidad de productos agregados para esa solución parcial. Si sumando a este valor, la cantidad de productos que no se revisaron aún (es decir, suponiendo que se pueden agregar todos los productos que faltan), no puedo superar al máximo encontrado, entonces no voy a llegar a una solución óptima por este subárbol. Podemos retornar 0 y así evitar el cómputo innecesario de operaciones.En el Algoritmo~\ref{alg:backtracking} se actualiza la variable $maxValue$ cada vez que se halla una mejor solución factible en la línea \ref{linea:opt_actualizar}, y se evalúa la regla de la poda en la línea \ref{linea:opt}.

\begin{algorithm}
\begin{algorithmic}[1]
\State $maxValue \gets 0$
\Function{$BTPodas$}{$i$, $k$, $p$}
   \If{$p > R$}
   {\textbf{return} $0$}
   \EndIf
   \If{$ \textbf{not} {todosLosProductosResistenPeso}(i, p)$}
   {\textbf{return} $0$}
   \EndIf
   \If{$k + (n - i) <= maxValue$}
   {\textbf{return} $0$}
   \EndIf
   \If{$i = n$}
        \If{$k > maxValue$}
        {$maxValue \gets k$}
        \EndIf
        \State \textbf{return} $k$
    \EndIf
    \State {$solucionParcial[i] \gets false$}
    \State {$sinAgregar \gets BTPodas(i+1, k, p)$}
    \State {$solucionParcial[i] \gets true$}
    \State {$agregado \gets BTPodas(i+1, k + 1, p + productos[i].peso)$}
    \State \textbf{return} $\max \{ sinAgregar,agregado \}$.
\EndFunction
\end{algorithmic}
\caption{Algoritmo de Backtracking con podas para jambo-tubos.}
\label{alg:backtracking}
\end{algorithm}

El algoritmo presentado de backtracking es correcto debido a que es exactamente el mismo que el utilizado en fuerza bruta (que ya fue demostrado que es correcto porque genera todas las soluciones candidatas posibles) con el agregado de podas, que ya fueron explicadas por qué son correctas. Notar que cuando se llega a una hoja de este árbol generado ($i$ $=$ $n$) no es necesario preguntar si la solución encontrada es válida, ya que las podas se usan en todos los nodos (tanto los intermedios como los nodos hojas), por lo que si se llega a la condicion de que $i$ $=$ $n$, ya está garantizado que el peso acumulado $p$ no supera la resistencia del tubo $R$ y que todos los productos desde el primero hasta el último resisten el peso por encima, que es lo que validaba la función de $esSolucionValida$. La solucion al problema tambien se obtiene llamando a la funcion $BTPodas(0, 0 , 0)$

La complejidad del algoritmo en el peor caso es $O(n*2^n)$. Esto se debe a que el algoritmo genera el mismo árbol que fuerza bruta, pero ahora la diferencia es que en cada nodo, tanto intermedio como hoja, se llama a la funcion para verificar si todos los productos resisten el peso, que ya se demostró que tiene una complejidad temporal de $O(n)$ en el peor caso. La cantidad de nodos internos en un arbol completo binario de $n + 1$ niveles es $2^n$ por lo que la cantidad de operaciones seria $2^n*n$ para todos los nodos intermedios y $2^n*n$ para los nodos hoja también. Por lo tanto la complejidad resultante es $O(n*2^n)$. No se ha podido identificar igualmente un caso o un tipo de instancias en particular en donde ninguna de las podas se utilice. Para los casos en donde ningún producto se puede agregar debido a que todos superan la resistencia del tubo, se resuelve de manera muy eficiente ya que la primera poda que verifica que el peso acumulado no supera la resistencia hace que no se revise ningun subarbol en donde se agregue un producto. Los casos en donde se agregan pocos porque muy pocos resisten el peso encima, tambien se resuelven de manera eficiente por la poda que verifica que si algun producto agregado ya no soporta el peso, entonces no se siga por ese subarbol. Y los casos en donde muchos productos se pueden agregar (o todos se pueden agregar) la poda de optimalidad es util, ya que una vez que se encontro una solucion donde se agregaron $maxValue$ productos, todas las instancias en donde no se van a agregar mas de $maxValue$ productos, ya no se revisan.

\section{Programación Dinámica} \label{sec:dp}
La técnica de \emph{Programación Dinámica} se utiliza cuando un problema cumple con la propiedad de superposición de subproblemas. Es decir, cuando en el cálculo recursivo de subproblemas de un algoritmo, hay muchos que se repiten. La idea consiste en evitar recalcular todo el subárbol correspondiente si ya fue hecho con anterioridad. Para esto, primero hay que definir una función recursiva que resuelva al problema y luego ver que se cumple la propiedad de superposición de problemas. La función recursiva propuesta para este problema de los jambo-tubos es la siguiente: 

\begin{equation} \label{eq:dp}
    f(i, r) = \begin{cases}
        0 & \text{si } i = 0,\\
        f(i -1, r) & \text{si } i > 0 \land (w_i > r \lor r_i < R - r),\\
        \max \{ f(i-1, r), f(i-1, r - w_i) + 1 \} & \text{caso contrario. }
    \end{cases}
\end{equation}

La semántica de la función $f(i, r)$ es la siguiente: ``máxima cantidad de productos que se pueden apilar sin aplastarse del subconjunto de productos $\{S_1, \hdots, S_i\}$ dada la resistencia restante de peso del tubo $r$''. Notar que el peso que soporta cada producto se puede calcular mediante la cuenta $R-r$ ya que la resistencia inicial de peso del tubo $R$, menos la resistencia que queda cuando se van agregando productos $r$, da como resultado el peso agregado. La solución al problema, dadas estas definiciones, es claramente $f(n, R)$. A continuación se dará una breve explicación de por qué la definición de esta función es correcta.

\paragraph{Correctitud}
\begin{enumerate}
    \item[(i)] {Si $i = 0$ estamos buscando cuantos productos se pueden apilar de un conjunto vacio de productos, por lo que la respuesta es 0. Es el caso base.}
    \item[(ii)] {Si $i > 0$ y ($w_i > r$ \lor $r_i < R - r$) entonces quiere decir que el peso del producto $i$ supera a la resistencia restante del tubo, o que la resistencia del producto $i$ es menor al peso que tiene por encima (que definimos que ese peso se puede calcular con $R - r$). En cualquiera de los dos casos, significa que el producto $i$ no se puede agregar. Entonces, se devuelve el llamado de $f(i-1,r)$, es decir ese producto no se agrega.}
    \item[(iii)] { En cualquier otro caso buscamos el maximo entre agregar o no el producto $i$, llamando a la misma funcion recursivamente, en un caso con los parametros $i - 1$, y $r$, que significa que el producto no se agrega, por lo que resistencia restante del tubo sigue siendo la misma. Y en el otro caso se llama a la misma funcion con el parametro $i - 1$ y el $r - w_i$, que significa que el producto se agrego, por lo que a la resistencia restante del tubo se le resta el peso del producto agregado. Notar que al término de la derecha se le suma 1 por haber agregado al $i$-ésimo producto. La recursion termina porque el parametro de $i$ que empieza en $n$, en cada paso siempre se le resta 1, por lo que en algun momento llegara a 0, que es el caso base}
\end{enumerate}

\paragraph{Memoización}
Para poder realizar la memoización de resultados es necesario verificar primero que se cumple la superposición de problemas. Si analizamos los parámetros que recibe la función (\ref{eq:dp}) vemos que son $i \in [0, \hdots, n]$ y $r \in [0, \hdots, R]$. El caso cuando $i = 0$ es el caso base. Por lo tanto, la cantidad posible de argumentos distintos con los cuales se puede llamar a la función está determinada por la combinación de ellos, que en este caso son $\Theta(n * R)$. La función en peor caso puede invocarse a sí misma dos veces decrementando el parámetro $i$, por lo que la cantidad de llamados recursivos es $\Theta(2^n)$. Si $n*R << 2^n$ entonces se cumple la propiedad. 
Se concluye entonces que es beneficioso agregar una memoria que recuerde cuando un caso ya fue resuelto y su correspondiente resultado, para calcular una sola vez cada uno de ellos y asegurarnos no resolver más de $\Theta(n * R)$ casos. El Algoritmo~\ref{alg:dp} muestra esta idea aplicada a la función (\ref{eq:dp}). En la línea~\ref{linea:memoizacion} se lleva a cabo el paso de memoización que solamente se ejecuta si el estado no había sido previamente computado.

\begin{algorithm}
\begin{algorithmic}[1]
\State $M_{ir} \gets -1$ for $i \in [0, n], w \in [0, R]$.
\Function{$DP$}{$i$, $w$}
    \IfThen{$i = 0$}{$M_{ir} = 0$}
    \If{$M_{ir} = -1$}
    	
    	\If{$productos[i-1].peso > r$ \textbf{or} $productos[i-1].resistencia < R - r$}\
    		$M_{ir} = PD(i-1,r)$
    	\Else{$M_{ir} = $max\{$PD(i-1, r), PD(i-1, r - productos[i-1].peso) + 1$\}}
    	\EndIf
    \EndIf
    \label{linea:memoizacion}
    \State \textbf{return} $M_{iw}$
\EndFunction
\end{algorithmic}
\caption{Algoritmo de Programación Dinámica para SSP.}
\label{alg:dp}
\end{algorithm}

La complejidad del algoritmo entonces está determinada por la cantidad de estados que se resuelven y el costo de resolver cada uno de ellos. Como mencionamos previamente, a lo sumo se resuelven $O(n * w)$ estados distintos, y como todas las líneas del Algoritmo~\ref{alg:dp} realizan operaciones constantes entonces cada estado se resuelve en $O(1)$. Como resultado, el algoritmo tiene complejidad $O(n*w)$ en el peor caso. Es importante observar que el diccionario $M$ se puede implementar como una matriz con acceso y escritura constante. Más aún, notar que su inicialización tiene costo $\Theta(n*w)$, por lo tanto, el mejor y peor caso de nuestro algoritmo va a tener costo $\Theta(n*w)$.

\section{Experimentación} \label{sec:experimentacion}
En esta sección se presenta los experimentos computacionales realizados para evaluar los distintos métodos presentados en las secciones anteriores. Las ejecuciones fueron realizadas en una workstation con CPU Intel~Core~i7 @ 2.8~GHz y 8~GB de memoria RAM, y utilizando el lenguaje de programación \emph{C++}.

\subsection{Métodos}
Las configuraciones y métodos utilizados durante la experimentación son los siguientes:
\begin{itemize}
    \setlength{\itemsep}{1pt}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0pt}
    \item \textbf{FB}: Algoritmo~\ref{alg:fuerza_bruta} de Fuerza Bruta de la Sección~\ref{sec:fuerza_bruta}.
    \item \textbf{BT}: Algoritmo~\ref{alg:backtracking} de Backtracking de la Sección~\ref{sec:backtracking}.
    \item \textbf{BT-F}: Algoritmo~\ref{alg:backtracking} con excepción de la línea~\ref{linea:opt}, es decir, solamente aplicando podas por factibilidad.
    \item \textbf{BT-O}: Similar al método BT-F pero solamente aplicando podas por optimalidad, o sea, descartando la línea~\ref{linea:fact} del Algoritmo~\ref{alg:backtracking}.
    \item \textbf{DP}: Algoritmo~\ref{alg:dp} de Programación Dinámica de la Sección~\ref{sec:dp}.
\end{itemize}

\subsection{Instancias}
Para evaluar los algoritmos en distintos escenarios es preciso definir familias de instancias conformadas con distintas características. Por ejemplo, el algoritmo de Backtracking como se menciona en la Sección~\ref{sec:backtracking} tiene familias que producen mejores y peores casos para el algoritmo. Primero, antes de enumerar los \emph{datasets}, se define la \emph{densidad} de una instancia como el cociente $\frac{\max S_i}{W}$, es decir, es una medida de cuántos números de $S$ se necesitan para sumar $W$. A menor densidad, los números de $S$ son más chicos en relación a $W$ y por lo tanto se necesitan más de ellos. Finalmente, los \emph{datasets} definidos se enumeran a continuación.
\begin{itemize}
    \setlength{\itemsep}{1pt}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0pt}
    \item \textbf{densidad-alta}: En esta familia cada instancia tiene los números $1, \hdots, n$ en $S$ en algún orden aleatorio y se toma $W = \frac{n}{2}$.
    \item \textbf{densidad-baja}: Para esta conjunto de instancias se toman los números $1, \hdots, n$ en $S$ en algún orden aleatorio y se toma $W = \frac{n (n-1)}{4}$, es decir, la mitad de la suma de todos los números.
    \item \textbf{bt-mejor-caso}: Cada instancia de $n$ elementos, está formada por $S = \{1, \hdots, 1, W\}$ y algún $W > n$. Son las instancias para el mejor caso de Backtracking definidas en la Sección~\ref{sec:backtracking}.
    \item \textbf{bt-peor-caso}: Cada instancia de $n$ elementos, está formada por $S = \{1, \hdots, 1, 1\}$ y $W = n$. Son las instancias para el peor caso de Backtracking definidas en la Sección~\ref{sec:backtracking}.
    \item \textbf{dinamica}: Esta familia de instancias tiene instancias con distintas combinaciones de valores para $n$ y $W$ en los intervalos $[1000, 8000]$. Los números en $S$ son una permutación de el conjunto $\{1, \hdots, n\}$.
\end{itemize}

\subsection{Experimento 1: Complejidad de Fuerza Bruta}
En este experimento se analiza la performance del método FB en distintos contextos. El análisis de complejidad realizado en la Sección~\ref{sec:fuerza_bruta} indica que el tiempo de ejecución para el mejor y peor caso es idéntico y es exponencial en función de $n$. Para contrastar empíricamente estas afirmaciones se evalúa FB utilizando los datasets densidad-alta y densidad-baja y se grafica los tiempos de ejecución en función de $n$. 

La Figura~\ref{fig:fb-comparacion-densidad} presenta los resultados del experimento, donde se puede apreciar que ambas curvas están solapadas para la mayoría de las instancias. El mensaje principal de este gráfico es que los tiempos de ejecución parecen no alterarse según la densidad de las instancias y seguir la misma curva de crecimiento sin importar las características de las mismas.

\begin{figure}
    \centering
    \begin{subfigure}{0.3\linewidth}
        \centering
        \includesvg[scale=0.3]{images/fb-densidades}
        \caption{Tiempo de ejecución del método FB sobre densidad-alta y densidad-baja.}
        \label{fig:fb-comparacion-densidad}
    \end{subfigure}
    \begin{subfigure}{0.3\linewidth}
        \centering
        \includesvg[scale=0.3]{images/fb-complejidad}
        \caption{Tiempo de ejecución contra complejidad esperada.}
        \label{fig:fb-complejidad-a}
    \end{subfigure}
    \begin{subfigure}{0.3\linewidth}
        \centering
        \includesvg[scale=0.3]{images/fb-correlacion}
        \caption{Correlación entre el tiempo de ejecución y la complejidad esperada.}
        \label{fig:fb-complejidad-b}
    \end{subfigure}
    \caption{Análisis de complejidad del método FB.}
    \label{fig:fb-complejidad}
\end{figure}

A continuación, tomamos la ejecución sobre el dataset densidad-alta y evaluamos cuál es su correlación con la complejidad estudiada en la Sección~\ref{sec:fuerza_bruta}, es decir, $O(2^n)$. En la Figura~\ref{fig:fb-complejidad-a} se ilustra el tiempo de ejecución de FB a la par de una función exponencial de $O(2^n)$. Por otro lado, para la Figura~\ref{fig:fb-complejidad-b} se enumeran las instancias $I_1, \hdots, I_k$ y para cada una se grafica el tiempo de ejecución real $T(I_i)$ contra el tiempo esperado $E(I_i) = 2_i$, es decir, su \emph{gráfico de correlación}.

Se puede ver que el tiempo de ejecución sigue claramente una curval exponencial y además la correlación con la función $2^n$ es positiva y casi perfecta. En particular, el índice de correlación de Pearson de ambas variables es $r \approx 0.999893$. Por lo tanto, podemos afirmar que el algoritmo se comporta como se describió inicialmente en las hipótesis.

\subsection{Experimento 2: Complejidad de Backtracking}
En esta experimentación vamos a contrastar las hipótesis de la Sección~\ref{sec:backtracking} con respecto a las familias de instancias de mejor y peor caso para el Algoritmo~\ref{alg:backtracking}, y su respectiva complejidad. Para esto evaluamos el método BT con respecto los datasets bt-mejor-caso y bt-peor-caso. 

\begin{figure}
    \centering
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includesvg[scale=0.4]{images/bt-complejidad-mejor-caso}
        \caption{Tiempo de ejecución vs Complejidad esperada.}
        \label{fig:bt-complejidad-mejor-caso-a}
    \end{subfigure}
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includesvg[scale=0.4]{images/bt-correlacion-mejor-caso}
        \caption{Gráfico de correlación entre el tiempo de ejecución y la complejidad esperada.}
        \label{fig:bt-complejidad-mejor-caso-b}
    \end{subfigure}
    \caption{Análisis de complejidad del método BT para el data set bt-mejor-caso.}
    \label{fig:bt-complejidad-mejor-caso}
\end{figure}

Las Figuras \ref{fig:bt-complejidad-mejor-caso} y \ref{fig:bt-complejidad-peor-caso} muestran los gráficos de tiempo de ejecución de BT y de correlación para cada dataset respectivamente. Efectivamente, las hipótesis presentadas anteriormente se cumplen para ambos casos. Por un lado, para las instancias de mejor caso se puede ver que efectivamente la serie de puntos muestra un crecimiento lineal aunque presenta cierto ruido. Uno de los motivos para este comportamiento es que al ser un comportamiento lineal, los tiempos de ejecución son muy bajos para incluso $n=1000$. Como resultado, cualquier interferencia en el sistema operativo o cambio de contexto puede causar una fluctuación indeseada y alterar los resultados. Sin embargo, el índice de correlación de Pearson es $r \approx 0.973844$ lo cuál muestra que hay una correlación positiva fuerte entre los tiempos de ejecución y una función lineal.

\begin{figure}
    \centering
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includesvg[scale=0.4]{images/bt-complejidad-peor-caso}
        \caption{Tiempo de ejecución vs Complejidad esperada.}
        \label{fig:bt-complejidad-peor-caso-a}
    \end{subfigure}
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includesvg[scale=0.4]{images/bt-correlacion-peor-caso}
        \caption{Gráfico de correlación entre el tiempo de ejecución y la complejidad esperada.}
        \label{fig:bt-complejidad-peor-caso-b}
    \end{subfigure}
    \caption{Análisis de complejidad del método BT para el data set bt-peor-caso.}
    \label{fig:bt-complejidad-peor-caso}
\end{figure}

Por otra parte, para las instancias de peor caso no se ve este comportamiento, y los tiempos de ejecución se presentan más ajustados a la curva de complejidad exponencial. Notemos que en este caso se ejecutaron instancias hasta $n=30$, número elegido para evitar que el tiempo de ejecución sea demasiado grande ($>10$ segundos). Para estas instancias el índice de correlación de Pearson es de $r \approx 0.999891$ contra una función exponencial con base 2.

\subsection{Experimento 3: Efectividad de las podas}
Naturalmente, surgen varias preguntas luego del experimento anterior. En particular, una de ellas es qué sucede en el medio del comportamiento lineal y exponencial, y qué factores afectan al algoritmo para ir transitando entre ambos escenarios. Una de las hipótesis es que el Algoritmo~\ref{alg:backtracking} mejora su funcionamiento dependiendo de la densidad de las instancias, es decir, de cuántos elementos de $S$ se necesitan para sumar $W$. Por ejemplo, si $W = 100$ y $S_i > 50$ para todo $i$, entonces al seleccionar dos elementos en una solución parcial del algoritmo, o bien se suma $W$ o bien se supera ese valor y la poda por factibilidad es ejecutada. Por otra parte, esto también incide en el funcionamiento de la poda por optimalidad ya que al encontrarse una solución con cardinal $C$, la altura del árbol de backtracking se reduce a $C$.

En este experimento se compara el funcionamiento de los métodos BT, BT-F y BT-O con respecto a los datasets densidad-alta y densidad-baja. La hipótesis es que para las instancias de alta densidad los algoritmos van a ser más eficientes que con aquellas de baja densidad.

\begin{figure}
    \centering
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includesvg[scale=0.4]{images/bt-podas-alta}
        \caption{Efectividad de las podas para densidad-alta.}
        \label{fig:bt-podas-alta}
    \end{subfigure}
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includesvg[scale=0.4]{images/bt-podas-alta-zoom}
        \caption{Efectividad de las podas con zoom para densidad-alta. }
        \label{fig:bt-podas-alta-zoom}
    \end{subfigure}
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includesvg[scale=0.4]{images/bt-podas-baja}
        \caption{Efectividad de las podas para densidad-baja.}
        \label{fig:bt-podas-baja}
    \end{subfigure}
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includesvg[scale=0.4]{images/bt-podas-baja-zoom}
        \caption{Efectividad de las podas con zoom para densidad-baja. }
        \label{fig:bt-podas-baja-zoom}
    \end{subfigure}
    \caption{Comparación de efectividad en las podas.}
    \label{fig:bt-podas}
\end{figure}

En la Figura~\ref{fig:bt-podas-alta} se muestra los resultados para el dataset densidad-alta. Se ejecutaron instancias hasta $n=200$ para evitar que el algoritmo demore más de unos segundos. Una observación interesante es que los tiempos de ejecución para los tres métodos están entre aquellos observados para los mejores y peores casos del algoritmo de BT. Por otra parte, es interesante que la poda por optimalidad tuvo mayor impacto que la poda por factibilidad. Esto puede deberse al hecho de que la densidad fue definida con respecto al máximo elemento $S$ pero sin tener en cuenta el resto, por lo tanto, puede ser que existan varios elementos que juntos sean menores a $W$ afectando la efectividad de la poda por factibilidad. Es interesante, sin embargo, mirar la Figura~\ref{fig:bt-podas-alta-zoom} que hace un acercamiento para evaluar mejor la diferencia entre BT y BT-O. En ella se aprecia que la combinación de ambas podas es más efectiva y es por esto que se puede concluir que la poda por factibilidad tiene un impacto positivo en el algoritmo final.

Por el lado de las instancias de baja densidad, los resultados están expuestos en la Figura~\ref{fig:bt-podas-baja}. El mensaje principal de estos gráficos es que a diferencia de las instancias de densidad alta, la efectividad de las podas no es lo suficientemente importante como para modificar de manera considerable el comportamiento del algortimo. En particular, esto se observa en el tamaño de las instancias ejecutadas (hasta $n = 30$) que si bien presentan tiempos más chicos que en las instancias de peor caso conservan su naturaleza exponencial. Como conclusión podemos afirmar que la densidad de las instancias tiene un peso significativo en el funcionamiento de este algoritmo.

\subsection{Experimento 4: Complejidad de Programación Dinámica}
A continuación se analiza la eficiencia del algoritmo de Programación Dinámica en la práctica y su correlación con la cota teórica calculada en la Sección~\ref{sec:dp}. Para esto, se ejecutan las instancias del dataset dinamica sobre el método DP y se grafican sus resultados en la Figura~\ref{fig:dp-tiempos}.

Las Figuras \ref{fig:dp-n} y \ref{fig:dp-W} muestran el crecimiento del tiempo de ejecución en función de $n$ y $W$ respectivamente, sobre algunos cortes hechos en la otra variable. Se puede ver que todas las líneas se comportan de manera similar, con un crecimiento lineal en función de ambas variables. Esto se reafirma en la Figura~\ref{fig:dp-NW} donde se muestra el crecimiento del tiempo de ejecución en función de ambas variables al mismo tiempo. Allí se puede apreciar que el crecimiento es similar tanto en la dirección de $n$ como en $W$. Finalmente, para confirmar que el tiempo de ejecución de nuestro algoritmo es efectivamente $O(nW)$ como se hipotetiza en la Sección~\ref{sec:dp}, se exhibe un gráfico de correlación a lo largo de todas las instancias comparando el tiempo de ejecución contra el tiempo esperado. Este gráfico muestra una correlación positiva bastante fuerte entre ambas series de datos, lo cuál es confirmado por el índice de correlación de Pearson que es $r \approx 0.996075$.

\begin{figure}
    \centering
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includesvg[scale=0.4]{images/dp-n}
        \caption{Tiempo de ejecución en función de $n$.}
        \label{fig:dp-n}
    \end{subfigure}
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includesvg[scale=0.4]{images/dp-W}
        \caption{Tiempo de ejecución en función de $W$.}
        \label{fig:dp-W}
    \end{subfigure}
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includesvg[scale=0.4]{images/dp-heatmap}
        \caption{Tiempo de ejecución en función de $n$ y $W$.}
        \label{fig:dp-NW}
    \end{subfigure}
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includesvg[scale=0.4]{images/dp-correlacion}
        \caption{Correlación entre el tiempo de ejeución y la cota de complejidad temporal.}
        \label{fig:dp-complejidad}
    \end{subfigure}
    \caption{Resultados computacionales para el método DP sobre el dataset dinamica.}
    \label{fig:dp-tiempos}
\end{figure}

\subsection{Experimento 5: Backtracking vs Programación Dinámica}
Para finalizar, presentamos un experimento que compara dos técnicas algorítmicas distintas. La idea es obtener información que permita entender el comportamiento de cada método y que sirva para la toma de decisión al momento de elegir alguno. Nuestra hipótesis es que ambos algoritmos van a comportarse mejor en situaciones distintas. Por ejemplo, Backtracking funciona muy bien en las instancias de densidad alta, y sus podas pueden llegar a ser muy efectivas en comparación con el alto costo de mantenimiento de la estructura de memoización de programación dinámica. Sin embargo, cuando la densidad es baja programación dinámica debe ser más eficiente. 

Una observación importante es que ningún algoritmo \emph{domina} al otro en términos de complejidad. Dicho de otro modo, no es cierto que $O(2^n) \subseteq O(nW)$ ni tampoco que $O(nW) \subseteq O(2^n)$. Mirando con detenimiento ambas complejidades, podemos observar que el tiempo de ejecución de BT en el peor caso no depende de $W$, y por lo tanto, ante un valor muy grande el método DP debería degradarse de manera considerable incluso alcanzando un límite de memoria.

\begin{figure}
    \centering
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includesvg[scale=0.4]{images/comparacion-bt-dp-alta}
        \caption{Dataset densidad-alta.}
        \label{fig:comparacion-bt-dp-alta}
    \end{subfigure}
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includesvg[scale=0.4]{images/comparacion-bt-dp-baja}
        \caption{Dataset densidad-baja.}
        \label{fig:comparacion-bt-dp-baja}
    \end{subfigure}
    \caption{Comparación de tiempos de ejecución entre DP y BT.}
    \label{fig:comparacion-bt-dp}
\end{figure}

La Figura~\ref{fig:comparacion-bt-dp} muestra la comparación entre los métodos DP y BT para los datasets densidad-alta y densidad-baja. La hipótesis se confirma, mostrando que DP es más efectivo ante instancias de menor densidad que BT, aunque es robusto para ambos tipos de instancias. Notar que los tiempos de ejecución son bajos en ambos tipos de instancias para DP. El crecimiento de BT en las instancias de densidad baja, sin embargo, es claramente exponencial y hace que en este tipo de instancias la elección segura sea utilizar el algoritmo DP.


\section{Conclusiones} \label{sec:conclusiones}
En este trabajo se presentan tres algoritmos que usan técnicas distintas para resolver el SSP. El algoritmo de Fuerza Bruta es poco eficiente para resolver este problema ya que al aumentar el número de elementos de $S$ rápidamente crece su tiempo de ejecución a tiempos inmanejables. Una mejora a este algoritmo es el de Backtracking con sus podas que demuestran ser de utilidad en todas las instancias, y logran inclusive bajar el crecimiento de los tiempos de ejecución cuando las instancias poseen cierta estructura. Por último, el algoritmo de Programación Dinámica es el más robusto frente al crecimiento de la variable $n$, aunque es más sensible a el tamaño de $W$ lo que hace que ante valores de $W$ muy grandes no sea la mejor elección.

Una línea de trabajo futuro es analizar distintas estructuras de memoización para el método DP de manera tal de poder mitigar el uso de memoria para tamaños grandes de instancias. Esto puede ser combinado con otro tipo de implementación de índole iterativa. Por otra parte, las podas utilizadas en el algoritmo de Backtracking son las más simples para este problema, pero otras reglas más complejas pueden resultar de utilidad en otros contextos.
\end{document}